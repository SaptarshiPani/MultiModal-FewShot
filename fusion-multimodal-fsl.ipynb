{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10599180,"sourceType":"datasetVersion","datasetId":6560532}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Subset\nfrom timm import create_model\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sentence_transformers import SentenceTransformer\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Dataset and transforms\ndata_dir = \"/kaggle/input/breakhist-binary-classificationfew-shot/BreakHist/Final 200X\"\n\ntransform_vit = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntransform_clip = transforms.Compose([\n    transforms.Resize((224, 224)),\n])\n\n# Create datasets\ntrain_dataset_vit = ImageFolder(root=f\"{data_dir}/Train\", transform=transform_vit)\ntest_dataset_vit = ImageFolder(root=f\"{data_dir}/Test\", transform=transform_vit)\ntrain_dataset_clip = ImageFolder(root=f\"{data_dir}/Train\", transform=transform_clip)\ntest_dataset_clip = ImageFolder(root=f\"{data_dir}/Test\", transform=transform_clip)\n\n# Load models\nvit_model = create_model('vit_small_patch16_224', pretrained=True)\nvit_model.head = nn.Linear(vit_model.num_features, len(train_dataset_vit.classes))\nvit_model.to(device)\n\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\ntext_encoder = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n\n# Hyperparameters\nnum_epochs = 50\nlearning_rate = 0.001\nbatch_size = 32\nshots_per_class = 5\n\n# Feature fusion helper functions\ndef calculate_feature_relevance(features, labels):\n    \"\"\"\n    Calculate feature relevance using Fisher's criterion for each feature dimension\n    \"\"\"\n    unique_labels = np.unique(labels)\n    n_features = features.shape[1]\n    relevance_scores = np.zeros(n_features)\n    \n    for i in range(n_features):\n        feature_i = features[:, i]\n        \n        # Calculate mean and variance for each class\n        class_means = np.array([np.mean(feature_i[labels == label]) for label in unique_labels])\n        class_vars = np.array([np.var(feature_i[labels == label]) for label in unique_labels])\n        \n        # Fisher's criterion: inter-class variance / intra-class variance\n        between_class_var = np.var(class_means)\n        within_class_var = np.mean(class_vars)\n        \n        # Avoid division by zero\n        relevance_scores[i] = between_class_var / (within_class_var + 1e-10)\n    \n    # Normalize scores\n    return relevance_scores / (np.sum(relevance_scores) + 1e-10)\n\ndef calculate_feature_similarity(vit_features, clip_features):\n    \"\"\"\n    Calculate similarity between ViT and CLIP features using canonical correlation analysis\n    \"\"\"\n    # Normalize features\n    vit_norm = vit_features - np.mean(vit_features, axis=0)\n    clip_norm = clip_features - np.mean(clip_features, axis=0)\n    \n    # Calculate correlation matrix\n    corr_matrix = np.dot(vit_norm.T, clip_norm) / (vit_features.shape[0] - 1)\n    \n    # Calculate similarity scores\n    similarity_scores = np.abs(np.diag(corr_matrix))\n    return similarity_scores / (np.sum(similarity_scores) + 1e-10)\n\ndef weighted_feature_fusion(vit_features, clip_features, labels=None):\n    \"\"\"\n    Perform weighted feature fusion using relevance and similarity metrics\n    \"\"\"\n    # Calculate feature relevance if labels are provided (for training set)\n    if labels is not None:\n        vit_relevance = calculate_feature_relevance(vit_features, labels)\n        clip_relevance = calculate_feature_relevance(clip_features, labels)\n    else:\n        # For test set, use uniform relevance\n        vit_relevance = np.ones(vit_features.shape[1]) / vit_features.shape[1]\n        clip_relevance = np.ones(clip_features.shape[1]) / clip_features.shape[1]\n    \n    # Calculate feature similarity\n    similarity_scores = calculate_feature_similarity(vit_features, clip_features)\n    \n    # Combine metrics to create final weights\n    vit_weights = vit_relevance * np.mean(similarity_scores)\n    clip_weights = clip_relevance * np.mean(similarity_scores)\n    \n    # Normalize weights\n    vit_weights = vit_weights / (np.sum(vit_weights) + 1e-10)\n    clip_weights = clip_weights / (np.sum(clip_weights) + 1e-10)\n    \n    # Apply weighted fusion\n    fused_features = np.concatenate([\n        vit_features * vit_weights.reshape(1, -1),\n        clip_features * clip_weights.reshape(1, -1)\n    ], axis=1)\n    \n    return fused_features\n\ndef get_support_query_indices(dataset, shots_per_class):\n    class_indices = {label: [] for label in range(len(dataset.classes))}\n    for idx, (_, label) in enumerate(dataset):\n        class_indices[label].append(idx)\n    \n    support_indices, query_indices = [], []\n    for label, indices in class_indices.items():\n        np.random.shuffle(indices)\n        support_indices.extend(indices[:shots_per_class])\n        query_indices.extend(indices[shots_per_class:])\n    \n    return support_indices, query_indices\n\n# Get support and query indices\nsupport_indices, query_indices = get_support_query_indices(train_dataset_vit, shots_per_class)\n\n# Create subsets\nsupport_set_vit = Subset(train_dataset_vit, support_indices)\nquery_set_vit = Subset(train_dataset_vit, query_indices)\nsupport_set_clip = Subset(train_dataset_clip, support_indices)\nquery_set_clip = Subset(train_dataset_clip, query_indices)\n\n# Extract labels for support set\nsupport_labels = np.array([label for _, label in support_set_vit])\n\ndef generate_texts(dataset):\n    texts = []\n    for img, _ in tqdm(dataset, desc=\"Generating texts\"):\n        if isinstance(img, torch.Tensor):\n            img = transforms.ToPILImage()(img)\n        \n        inputs = clip_processor(\n            images=img,\n            text=\"Describe in as much detail as possible this histopathology image focusing on features distinguishing malignant and benign tumors.\",\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True\n        ).to(device)\n        \n        with torch.no_grad():\n            image_features = clip_model.get_image_features(pixel_values=inputs.pixel_values)\n            generated_text = \"Generated text description\"  # Placeholder\n        \n        texts.append(generated_text)\n    return texts\n\n# Generate and encode texts\nprint(\"Generating support set texts...\")\nsupport_texts = generate_texts(support_set_clip)\nprint(\"Generating query set texts...\")\nquery_texts = generate_texts(query_set_clip)\nprint(\"Generating test set texts...\")\ntest_texts = generate_texts(test_dataset_clip)\n\ndef encode_texts(texts):\n    return text_encoder.encode(texts, convert_to_tensor=True, device=device).cpu().numpy()\n\nprint(\"Encoding texts...\")\nsupport_text_features = encode_texts(support_texts)\nquery_text_features = encode_texts(query_texts)\ntest_text_features = encode_texts(test_texts)\n\n# Training loop\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(vit_model.parameters(), lr=learning_rate)\n\ndef train_model_on_support():\n    support_loader = DataLoader(support_set_vit, batch_size=batch_size, shuffle=True)\n    for epoch in range(num_epochs):\n        vit_model.train()\n        epoch_loss = 0\n        correct = 0\n        total = 0\n        \n        for images, labels in tqdm(support_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            images, labels = images.to(device), labels.to(device)\n            \n            outputs = vit_model(images)\n            loss = criterion(outputs, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            epoch_loss += loss.item()\n        \n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Acc: {100*correct/total:.2f}%\")\n\nprint(\"Training model on support set...\")\ntrain_model_on_support()\n\ndef extract_fused_features(loader, text_features, labels=None):\n    vit_features, all_labels = [], []\n    vit_model.eval()\n    \n    with torch.no_grad():\n        for i, (images, targets) in enumerate(tqdm(loader)):\n            images = images.to(device)\n            features = vit_model(images).cpu().numpy()\n            vit_features.append(features)\n            all_labels.append(targets.numpy())\n    \n    vit_features = np.vstack(vit_features)\n    all_labels = np.hstack(all_labels)\n    \n    # Apply weighted feature fusion\n    fused_features = weighted_feature_fusion(vit_features, text_features, labels)\n    return fused_features, all_labels\n\n# Prepare loaders\nsupport_loader = DataLoader(support_set_vit, batch_size=1, shuffle=False)\nquery_loader = DataLoader(query_set_vit, batch_size=1, shuffle=False)\ntest_loader = DataLoader(test_dataset_vit, batch_size=1, shuffle=False)\n\n# Extract fused features\nprint(\"Extracting support set features...\")\nsupport_fused, _ = extract_fused_features(support_loader, support_text_features, support_labels)\nprint(\"Extracting query set features...\")\nquery_fused, query_labels = extract_fused_features(query_loader, query_text_features)\nprint(\"Extracting test set features...\")\ntest_fused, test_labels = extract_fused_features(test_loader, test_text_features)\n\n# Calculate prototypes using fused features\nclass_prototypes = []\nfor label in np.unique(support_labels):\n    class_features = support_fused[support_labels == label]\n    class_prototypes.append(class_features.mean(axis=0))\nclass_prototypes = np.array(class_prototypes)\n\n# Evaluate using fused features\nall_features = np.vstack([query_fused, test_fused])\nall_labels = np.concatenate([query_labels, test_labels])\n\ncorrect = 0\nfor feature, label in zip(all_features, all_labels):\n    similarities = cosine_similarity([feature], class_prototypes)[0]\n    predicted = np.argmax(similarities)\n    correct += (predicted == label)\n\nprint(f\"Final accuracy: {100*correct/len(all_labels):.2f}%\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-16T07:51:56.334440Z","iopub.execute_input":"2025-02-16T07:51:56.334784Z","iopub.status.idle":"2025-02-16T07:52:55.964736Z","shell.execute_reply.started":"2025-02-16T07:51:56.334757Z","shell.execute_reply":"2025-02-16T07:52:55.963691Z"}},"outputs":[{"name":"stdout","text":"Generating support set texts...\n","output_type":"stream"},{"name":"stderr","text":"Generating texts: 100%|██████████| 10/10 [00:00<00:00, 77.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generating query set texts...\n","output_type":"stream"},{"name":"stderr","text":"Generating texts: 100%|██████████| 1399/1399 [00:17<00:00, 78.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generating test set texts...\n","output_type":"stream"},{"name":"stderr","text":"Generating texts: 100%|██████████| 604/604 [00:07<00:00, 81.08it/s]","output_type":"stream"},{"name":"stdout","text":"Encoding texts...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c14060475135484e9213f5e584f7a512"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/44 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81e9c514234a4061ac7ef545b9e91bfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/19 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa6f9a9ec1f46ff973b2c22ba23213a"}},"metadata":{}},{"name":"stdout","text":"Training model on support set...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Loss: 0.5896, Acc: 80.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50: 100%|██████████| 1/1 [00:00<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/50], Loss: 5.8440, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/50: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/50], Loss: 7.2101, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/50: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/50], Loss: 1.7024, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/50: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/50], Loss: 3.6750, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/50: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/50], Loss: 0.7946, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/50: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/50], Loss: 0.6550, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/50: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/50], Loss: 0.7473, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/50: 100%|██████████| 1/1 [00:00<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/50], Loss: 0.7753, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/50: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/50], Loss: 0.6512, Acc: 90.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/50: 100%|██████████| 1/1 [00:00<00:00,  9.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/50], Loss: 0.7200, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/50: 100%|██████████| 1/1 [00:00<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/50], Loss: 0.6475, Acc: 70.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/50: 100%|██████████| 1/1 [00:00<00:00,  9.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/50], Loss: 0.6913, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/50: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/50], Loss: 0.6372, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/50: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/50], Loss: 0.6666, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/50: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/50], Loss: 0.6219, Acc: 80.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/50: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/50], Loss: 0.6375, Acc: 50.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/50: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/50], Loss: 0.5988, Acc: 80.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/50: 100%|██████████| 1/1 [00:00<00:00,  8.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/50], Loss: 0.5990, Acc: 60.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/50: 100%|██████████| 1/1 [00:00<00:00,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/50], Loss: 0.5624, Acc: 80.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/50: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [21/50], Loss: 0.5470, Acc: 70.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/50: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [22/50], Loss: 0.4921, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/50: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [23/50], Loss: 0.4655, Acc: 80.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/50: 100%|██████████| 1/1 [00:00<00:00,  9.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [24/50], Loss: 0.3820, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/50: 100%|██████████| 1/1 [00:00<00:00,  9.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [25/50], Loss: 0.3054, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/50: 100%|██████████| 1/1 [00:00<00:00,  9.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [26/50], Loss: 0.2587, Acc: 90.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/50: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [27/50], Loss: 0.3258, Acc: 80.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/50: 100%|██████████| 1/1 [00:00<00:00,  9.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [28/50], Loss: 0.8586, Acc: 70.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/50: 100%|██████████| 1/1 [00:00<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [29/50], Loss: 0.0548, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/50: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [30/50], Loss: 0.8334, Acc: 70.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/50: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [31/50], Loss: 0.7610, Acc: 70.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/50: 100%|██████████| 1/1 [00:00<00:00,  9.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [32/50], Loss: 0.7779, Acc: 70.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/50: 100%|██████████| 1/1 [00:00<00:00,  9.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [33/50], Loss: 0.1342, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/50: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [34/50], Loss: 0.4574, Acc: 70.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/50: 100%|██████████| 1/1 [00:00<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [35/50], Loss: 0.4803, Acc: 70.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/50: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [36/50], Loss: 0.1427, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/50: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [37/50], Loss: 0.1960, Acc: 90.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/50: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [38/50], Loss: 0.2681, Acc: 90.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/50: 100%|██████████| 1/1 [00:00<00:00,  9.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [39/50], Loss: 0.1105, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/50: 100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [40/50], Loss: 0.0750, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/50: 100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [41/50], Loss: 0.1299, Acc: 90.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/50: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [42/50], Loss: 0.1147, Acc: 90.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/50: 100%|██████████| 1/1 [00:00<00:00,  9.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [43/50], Loss: 0.0383, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/50: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [44/50], Loss: 0.0197, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/50: 100%|██████████| 1/1 [00:00<00:00,  9.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [45/50], Loss: 0.0407, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/50: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [46/50], Loss: 0.0465, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/50: 100%|██████████| 1/1 [00:00<00:00,  9.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [47/50], Loss: 0.0150, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/50: 100%|██████████| 1/1 [00:00<00:00,  9.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [48/50], Loss: 0.0040, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/50: 100%|██████████| 1/1 [00:00<00:00,  9.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [49/50], Loss: 0.0021, Acc: 100.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/50: 100%|██████████| 1/1 [00:00<00:00,  9.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [50/50], Loss: 0.0038, Acc: 100.00%\nExtracting support set features...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 108.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting query set features...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1399/1399 [00:14<00:00, 96.76it/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting test set features...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 604/604 [00:06<00:00, 98.70it/s] \n","output_type":"stream"},{"name":"stdout","text":"Final accuracy: 87.12%\n","output_type":"stream"}],"execution_count":20}]}